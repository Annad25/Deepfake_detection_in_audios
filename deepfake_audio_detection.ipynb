{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part 3: Documentation & Analysis",
      "metadata": {
        "id": "HizxJga98sI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Momenta Audio Deepfake Detection\n- **Model**: [mo-thecreator/Deepfake-audio-detection](https://huggingface.co/mo-thecreator/Deepfake-audio-detection) (wav2vec2-base)\n- **Dataset**: [Hemg/Deepfake-Audio-Dataset](https://huggingface.co/datasets/Hemg/Deepfake-Audio-Dataset)\n- **Goal**: Fine-tuned for deepfake detection",
      "metadata": {
        "id": "6vlrhlmf8gdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Implementation Process\n\n### Challenges Encountered\n- **Small Dataset**: The Hemg dataset only had 100 samples—splitting it left me with a tiny validation set (10 samples).\n- **Audio Mismatch**: Some clips weren’t at 16 kHz, which wav2vec2-base needs, so I had to resample them.\n- **Overfitting Risk**: Training loss dropped fast (0.0002 by Epoch 10), but validation loss bounced around—model might be memorizing too much.",
      "metadata": {
        "id": "Umr3F4lV8jJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### How I Addressed These Challenges\n- **Dataset Split**: Used `train_test_split` to make 90 train and 10 test samples. Kept it small by sticking to the full 100 clips—didn’t need to subset further.\n- **Resampling**: Added a `torchaudio` resampler in `preprocess_function` to fix sampling rates to 16 kHz—matched the model’s expectations.\n- **Overfitting Fix**: Added noise and time-shifting in preprocessing to shake up the data. Set `load_best_model_at_end=True` to grab Epoch 2’s model (lowest val loss: 0.307129), avoiding later overfit ones.",
      "metadata": {
        "id": "L4BKtvvE8i6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Assumptions Made\n- Assumed 100 samples were enough to show my approach—small but balanced (real/fake split assumed from dataset).\n- Figured wav2vec2-base’s pre-training handled most audio patterns, so fine-tuning just tweaked it for deepfakes.\n- Thought 90% accuracy on 10 samples was fine for a demo.",
      "metadata": {
        "id": "TaHxCLvA9YUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Analysis\n\n### Why I Selected This Model\n- **Pre-Trained Power**: Grabbed [mo-thecreator/Deepfake-audio-detection](https://huggingface.co/mo-thecreator/Deepfake-audio-detection) from Hugging Face—wav2vec2-base is pre-trained on tons of audio, so I just fine-tuned it.\n- **Task Fit**: Built for deepfake detection, perfect for AI-generated speech. Good for convos (speech-focused), and with tweaks, it could run near real-time.\n",
      "metadata": {
        "id": "eOVdO7L39YRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### How the Model Works\n- **Big Picture**: Takes raw audio, processes it with wav2vec2-base (CNN + Transformer), and spits out “real” or “fake.”\n- **Steps**:\n  1. Audio hits at 16 kHz—I resampled if needed.\n  2. CNN pulls out sound bits (like voice patterns).\n  3. Transformer ties it all together, spotting fake clues over time.\n  4. Fine-tuning teaches it my dataset’s real/fake labels.\n- **Simple Take**: It’s a smart listener that learns to catch fakes",
      "metadata": {
        "id": "J9JgTZnc9YIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Performance Results\n- **Best Run**: Epoch 2—Train Loss: 0.300600, Val Loss: 0.307129, Accuracy: 90%.\n- **Full Run**: 10 epochs, train loss dropped to 0.0002, val loss hovered ~0.7, accuracy stuck at 90% (9/10 right).\n",
      "metadata": {
        "id": "dnbp0WS_9YEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Observed Strengths\n- **Good results**: Pre-trained + fine-tuning = 90% accuracy fast.\n- **Audio identification**: Wav2vec2-base knows speech, great for AI fakes.\n- **Augmentation**: Noise and shifting helped it learn better early on.\n- **Best Model**: We saved the best model weights instead of using over fitted model.",
      "metadata": {
        "id": "kJQKVOvi9X2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Observed Weaknesses\n- **Tiny Test Set**: Only 10 validation samples.\n- **Overfitting**: Train loss crashed, but val loss jumped—model memorized too much by Epoch 10.\n\n",
      "metadata": {
        "id": "ih7zlg_QBr21"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Suggestions for Future Improvements\n- **Bigger Data**: Grab more Hemg samples or mix in ASVspoof2019 for a bigger test set.\n- **Stop Early**: Use early stopping (e.g., 3 epochs) to avoid overfitting, Epoch 2 was peak.\n- **Speed Boost**: Shrink the model (like with quantization) for real-time use.\n- **Noise Prep**: Train with messy audio (e.g., background chatter) for real convos.",
      "metadata": {
        "id": "utegtbdMB6-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Reflection Questions\n\n#### 1. What Were the Most Significant Challenges in Implementing This Model?\n- **Small Dataset Size**: The Hemg dataset had only 100 samples—splitting it into 90 train and 10 test made validation wobbly. Hard to trust 90% accuracy with so few test clips!\n- **Audio Prep**: Some clips weren’t at 16 kHz (wav2vec2-base’s need), so resampling with `torchaudio` was a must.\n\n\n#### 2. How Might This Approach Perform in Real-World Conditions vs. Research Datasets?\n- **Research Datasets (like Hemg)**: My 90% accuracy looks good, but Hemg’s clean, controlled clips (100 samples) made it easier. Wav2vec2-base’s pre-training helped nail patterns in this small, neat set.\n- **Real-World Conditions**: Real-world audio with noise, varying lengths, and accents could challenge my model due to overfitting on clean data and slow inference, despite some help from noise augmentation.\n\n#### 3. What Additional Data or Resources Would Improve Performance?\n- **More Data**: Bigger dataset—like ASVspoof2019’s LA subset would give more real/fake variety.\n- **Noisy Audio**: Clips with real-world noise (cafés, streets) to train for robustness.\n- **Compute Power**: A better high performance GPU or TPU for faster training on larger sets.\n- **Model Details**: Full architecture docs from Hugging Face—tweaking wav2vec2-base layers could boost it.\n\n#### 4. How Would You Approach Deploying This Model in a Production Environment?\n- **Optimize Speed**: Shrink it—use quantization (e.g., `torch.quantization`) or pruning to cut the Transformer’s heft. Aim for <100 ms inference per clip for near real-time.\n- **API Setup**: Wrap it in a Flask or FastAPI server—endpoint takes audio, resamples to 16 kHz, runs inference, spits out “real” or “fake.” Host on AWS/GCP with GPU support.\n- **Preprocessing Pipeline**: Automate resampling and augmentation in a stream—handle live audio chunks with a buffer.\n- **Monitoring**: Add logging for false positives/negatives—retrain monthly with new fakes to keep it sharp. Test with a noisy convo dataset first.",
      "metadata": {
        "id": "cvytWI_BB65P"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Requirements\n\n### Clear Setup Instructions\n- **Environment**: Use Google Colab (free GPU tier, T4 recommended).\n- **Steps**:\n  1. Open Colab: Go to [colab.research.google.com](https://colab.research.google.com), click “New Notebook.”\n  2. Enable GPU: Runtime > Change runtime type > GPU > Save.\n  3. run this '!pip install transformers datasets torchaudio librosa evaluate'\n\n### Document Any Dependencies\n- **Packages** (installed in code):\n  - `transformers`: For loading wav2vec2-base model and feature extractor.\n  - `datasets`: To fetch and process Hemg/Deepfake-Audio-Dataset.\n  - `torchaudio`: For resampling audio to 16 kHz.\n  - `librosa`: Audio processing (used in resampling).\n  - `evaluate`: Accuracy metric for training.\n  - `torch`: PyTorch for model and GPU support.\n- **Versions**: Pinned in Colab’s default env\n### Ensure Reproducibility\n- **Access to Data**:\n  - **Dataset**: [Hemg/Deepfake-Audio-Dataset](https://huggingface.co/datasets/Hemg/Deepfake-Audio-Dataset).\n  - **Instructions**: Code uses `datasets.load_dataset(\"Hemg/Deepfake-Audio-Dataset\")`—automatically pulls 100 samples from Hugging Face Hub. No manual download needed; just run the cell.\n- **Model**: Pre-trained [mo-thecreator/Deepfake-audio-detection](https://huggingface.co/mo-thecreator/Deepfake-audio-detection) loaded via `transformers`—publicly available on Hugging Face.\n\n- **Code**: Full script below—run as-is.",
      "metadata": {
        "id": "P4Rb6nqkB60h"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "id": "c4tyNzZKB6pS"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "QXlA0ypK8c-c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "!pip install transformers datasets torchaudio librosa evaluate\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZCmaJ0G7U-G",
        "outputId": "48ee5fc3-aba2-41ef-d0c7-a4fc59f21408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\nimport evaluate\nimport torch\nimport torchaudio\n\n# Check for GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# 1. Load the Deepfake Audio Dataset from Hugging Face Hub\ndataset = load_dataset(\"Hemg/Deepfake-Audio-Dataset\")\nprint(\"Original dataset splits:\", dataset)\n\n# 2. Load the Pretrained Audio Classification Model and its Feature Extractor\nmodel_name = \"mo-thecreator/Deepfake-audio-detection\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\nmodel = AutoModelForAudioClassification.from_pretrained(model_name).to(device)\n\n# Target sampling rate for the model\nTARGET_SR = 16000\n\n# 3. Preprocess the Dataset with resampling if necessary.\ndef preprocess_function(example):\n    audio_array = example[\"audio\"][\"array\"]\n    orig_sr = example[\"audio\"][\"sampling_rate\"]\n\n    if orig_sr != TARGET_SR:\n        audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=TARGET_SR)\n        audio_tensor = resampler(audio_tensor)\n        audio_array = audio_tensor.numpy()\n        orig_sr = TARGET_SR\n\n    processed = feature_extractor(\n        audio_array,\n        sampling_rate=orig_sr,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    processed = {key: value.squeeze(0) for key, value in processed.items()}\n    processed[\"labels\"] = example[\"label\"]\n    return processed\n\n# Apply the preprocessing function to the dataset\nencoded_dataset = dataset.map(preprocess_function)\n\n# Since we only have a \"train\" split, split it into training and validation sets.\nsplit_dataset = encoded_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\nprint(\"Splits after train_test_split:\", split_dataset)\n\n# 4. Define the evaluation metric\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return accuracy_metric.compute(predictions=preds, references=labels)\n\n# 5. Set up training arguments with eval_strategy instead of evaluation_strategy\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=[]  # Disable reporting to wandb and other trackers\n)\n\n# 6. Initialize the Trainer using the new splits\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\n\n# 7. Fine-tune the model\ntrainer.train()\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942,
          "referenced_widgets": [
            "20c0cd6601a04cf6a184f4d80dc650ce",
            "1f270c8cf77a4acba384c39d61d616c0",
            "b91793f4d9ca4f7f906d2644d7eb595b",
            "dc43cceacfb24209b7657e5d4deaebbc",
            "fa0fc841be1d4516b32faa3fd4f573ef",
            "0a377071b69d469eb8563f74f285f8c9",
            "ca4610c4f664401e82d4d76e97301f1c",
            "448ac64ff2884968a6d724ffcc937703",
            "b2bb3b8a608e463ba2e1cec9fb634ed4",
            "a5ae9250363d4bf5a267940029505dbc",
            "56dbc9feada74e439b3bd57c7b6cf125",
            "e82f4dbe7fca41a390411bf30faa8adc",
            "e9801349d8d8494d8d991a088ce7ccab",
            "10426b03bc1146728e7a5d1dfc7ef2b3",
            "24cc5c0e966e40dfa84ba826d856f8ce",
            "96b24d6edd3a4529aff297a4a575dead",
            "8446f2d1b01a4c75950fa3bb3b392e79",
            "e406c40388fe4003b305625a009dd1b3",
            "27fd7c6c28354d6e84ea8a6ec87f21f6",
            "fe7e3c7a12e9453689879450e445d29b",
            "496af587b7cf4d4b9e7734b01c072d61",
            "9eab5d52c203440aa8d9e70e2485c91a",
            "eb9f4a3561e54b079b1e5406c655ad36",
            "ffb86d8c3a80465e921bfcc7c3da8bd8",
            "cb43159058514bcda130c6ab03d19add",
            "b36da7e0ae04497a9069b8eb8a1012ae",
            "ed14b2ca789a4a4782dd0bcf59e24cab",
            "f8774531492c459fac76300b3a9d798a",
            "170b5350f0234e7da00f22e1b8db6d3b",
            "6cf925239c954d51aa6c45f25c9a6865",
            "bf57425279614fbbad252debccc787ba",
            "fdd81971f5054e4e8fcc5a0c4dd101b7",
            "de2d0c67d1b34f6e98877ecec93df4b7",
            "d9ab3114074040fc88638abf8e6a8c16",
            "21f99fec2ab14363bcb5015d68befaf9",
            "e6391aecc0b74edca558b618d88beed3",
            "744561d308004f82af3cc98a0e892d45",
            "a3854708b220422ea4e64cb0151db4d2",
            "23a4cb72c6814ff980f1f44eae1156d2",
            "a7d041e4259b4e1284bef3e28f81663d",
            "75579f0ca14c4272849f7fe2da239aac",
            "939e26d7da13489a9455db44460fe460",
            "9969f8d8d3ea443da182cd131552b7dc",
            "d74f910267d54c40b841aadabceb7c44",
            "21bee97500d04d53920bdadc5319e30c",
            "1fc93e6afc3745558a0cd65a62d667aa",
            "943a8a239e7249b8ae864ae2dba19b65",
            "2f243490bc1f4a77a0697104de03fb36",
            "603c6ba1d81e4c2eb79de490d7db1310",
            "8513622f094c4bf1bfb53f9a7e56d975",
            "25f202cefecd449abe75a18ef1f14e48",
            "fe88753e747849dba708d9a0263e6339",
            "aee3952e669e4244a61b3746600669ff",
            "5064ec765a8845bd9d7da6a6d870a589",
            "effc6f9202a74c96a7e215a4f6236f01",
            "270e05407579455d84acad1955e88d85",
            "70ebc638e5ff4004adebbad3cf7c8881",
            "ea6bbf3267c643618567c3c5a99ab207",
            "7d7be011acad4c459ddcd8c7d588143f",
            "c5a6421caa8243b08d194a05083b03d6",
            "f21d168bdfd243f8ad018371ea1660c2",
            "43784a42900a4f34a52c47a311784ab1",
            "32fab9a6c801414c918c252070da2fcf",
            "e2fdf67b50b6484cbee446b2e25e7782",
            "96391a522a3b489a941fb2b0afdfb9ca",
            "945617d2f23f4ba8a7ffaaaac679961d",
            "b6390f3f3d2d457d87701ef0a1f747eb",
            "0033371a22bd4dcf9bf2f4b0e5da0d0d",
            "a6f6eec3e91c4508892dac77e9a4f836",
            "3ca58969b80242b4a19c296df7a2e356",
            "32921938d5894ebb94a33ae32f580958",
            "9ef203fec6ca48abac56530656754d3f",
            "edbf34317ec4491297a0aa5a8a4cc75e",
            "7c89d8b0456143a9928e0f58da066677",
            "c95e4da794b54c56a2f099b25d5f4782",
            "e665593d8082463594174f794294091c",
            "37a1b89fdee34deeb9e36d2198f6dfd9",
            "db2732d28fd94826b38796922ecf0b52",
            "fa059fd7cff64dd9bf39f624562f5cbd",
            "4043e8c57cc340a086402b55e9de6820",
            "8e64ff56fc9e460e9b1e8f315db2f46d",
            "ae13d9b6e68049629432ea891e45c928",
            "c7d3fbc8516e4179997eceda66d9ede6",
            "a1e1fb983b7943b291b82b9959dc6d1c",
            "c45e7d11fe0a4496b024da132853cd21",
            "77ba71f48e82451bbb07f4d3b143341c",
            "3868aac98e0f4ec8aa7db85404d2f67d",
            "2fcb191c6c614f4c86ef3db59b2dd2c7"
          ]
        },
        "id": "QNH1qXaBGXiI",
        "outputId": "ee18f387-d2bc-4679-9118-65e394056910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/557 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20c0cd6601a04cf6a184f4d80dc650ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)-00000-of-00001-ab2dff7d513c15ff.parquet:   0%|          | 0.00/85.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e82f4dbe7fca41a390411bf30faa8adc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb9f4a3561e54b079b1e5406c655ad36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset splits: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'label'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9ab3114074040fc88638abf8e6a8c16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21bee97500d04d53920bdadc5319e30c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "270e05407579455d84acad1955e88d85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6390f3f3d2d457d87701ef0a1f747eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits after train_test_split: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'label', 'input_values', 'labels'],\n",
            "        num_rows: 90\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'label', 'input_values', 'labels'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db2732d28fd94826b38796922ecf0b52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.380000</td>\n",
              "      <td>1.052455</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.786600</td>\n",
              "      <td>1.220167</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>1.241462</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>1.852199</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.098400</td>\n",
              "      <td>1.500057</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=0.6715348651011784, metrics={'train_runtime': 151.2138, 'train_samples_per_second': 2.976, 'train_steps_per_second': 0.397, 'total_flos': 4.085384688e+16, 'train_loss': 0.6715348651011784, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\nimport evaluate\nimport torch\nimport torchaudio\nimport random\n\n# Check for GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# 1. Load the Deepfake Audio Dataset from Hugging Face Hub\ndataset = load_dataset(\"Hemg/Deepfake-Audio-Dataset\")\nprint(\"Original dataset splits:\", dataset)\n\n# 2. Load the Pretrained Audio Classification Model and its Feature Extractor\nmodel_name = \"mo-thecreator/Deepfake-audio-detection\"\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\nmodel = AutoModelForAudioClassification.from_pretrained(model_name).to(device)\n\n# Target sampling rate for the model\nTARGET_SR = 16000\n\n\n\ndef preprocess_function(example):\n    audio_array = example[\"audio\"][\"array\"]\n    orig_sr = example[\"audio\"][\"sampling_rate\"]\n\n    # === Data Augmentation ===\n    # Noise injection\n    noise_level = 0.005\n    noise = np.random.randn(len(audio_array))\n    audio_array = audio_array + noise_level * noise\n\n    # Time shifting\n    shift_range = int(0.1 * orig_sr)  # shift up to 100ms\n    shift = random.randint(-shift_range, shift_range)\n    if shift > 0:\n        audio_array = np.concatenate((audio_array[shift:], np.zeros(shift)))\n    else:\n        audio_array = np.concatenate((np.zeros(-shift), audio_array[:shift]))\n\n    # === Resample ===\n    if orig_sr != TARGET_SR:\n        audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=TARGET_SR)\n        audio_tensor = resampler(audio_tensor)\n        audio_array = audio_tensor.numpy()\n        orig_sr = TARGET_SR\n\n    processed = feature_extractor(\n        audio_array,\n        sampling_rate=orig_sr,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    processed = {key: value.squeeze(0) for key, value in processed.items()}\n    processed[\"labels\"] = example[\"label\"]\n    return processed\n# Apply the preprocessing function to the dataset\nencoded_dataset = dataset.map(preprocess_function)\n\n# Since we only have a \"train\" split, split it into training and validation sets.\nsplit_dataset = encoded_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\nprint(\"Splits after train_test_split:\", split_dataset)\n\n# 4. Define the evaluation metric\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return accuracy_metric.compute(predictions=preds, references=labels)\n\n# 5. Set up training arguments with eval_strategy instead of evaluation_strategy\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,  # increased from 5 to 10\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,               #  This reloads best model after training ends\n    metric_for_best_model=\"accuracy\",          #  This decides how “best” is defined\n    greater_is_better=True,                    #  For metrics like accuracy, higher is better\n    save_total_limit=1,                        #  limits number of checkpoints\n    report_to=[]  # Disable external logging\n)\n\n\n# 6. Initialize the Trainer using the new splits\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\n\n# 7. Fine-tune the model\ntrainer.train()\n\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "7tQsyUNZRWyP",
        "outputId": "282cf000-4646-435a-ab9f-04202f732ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Original dataset splits: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'label'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "})\n",
            "Splits after train_test_split: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'label', 'input_values', 'labels'],\n",
            "        num_rows: 90\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'label', 'input_values', 'labels'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 05:15, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.540100</td>\n",
              "      <td>0.437007</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.300600</td>\n",
              "      <td>0.307129</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>0.756505</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.346500</td>\n",
              "      <td>0.750608</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.740430</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.141100</td>\n",
              "      <td>0.654343</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.706897</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.718901</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.723980</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.729421</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=120, training_loss=0.2065585005407532, metrics={'train_runtime': 317.8831, 'train_samples_per_second': 2.831, 'train_steps_per_second': 0.377, 'total_flos': 8.170769376e+16, 'train_loss': 0.2065585005407532, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# Save best model to disk\ntrainer.save_model(\"./best_model\")\n",
      "metadata": {
        "id": "zdzblWGjbMK0"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "!zip -r results.zip ./results\n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKfpD3SAbMgK",
        "outputId": "ce6af738-7ebb-4700-b8ab-982061befaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: results/ (stored 0%)\n",
            "  adding: results/checkpoint-12/ (stored 0%)\n",
            "  adding: results/checkpoint-12/trainer_state.json (deflated 58%)\n",
            "  adding: results/checkpoint-12/scheduler.pt (deflated 56%)\n",
            "  adding: results/checkpoint-12/model.safetensors (deflated 7%)\n",
            "  adding: results/checkpoint-12/training_args.bin (deflated 52%)\n",
            "  adding: results/checkpoint-12/optimizer.pt (deflated 7%)\n",
            "  adding: results/checkpoint-12/config.json (deflated 66%)\n",
            "  adding: results/checkpoint-12/rng_state.pth (deflated 25%)\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "Bg2gKD3xbMjQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "Q7_Xgv1nbMl4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "PPEIjNizbMos"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}